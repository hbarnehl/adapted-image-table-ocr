{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Table Scanner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "improving cell detection and ocr by changing kernel values\n",
    "\n",
    "In the original modules, the smallest cells were not detected because they were smaller than the minimum threshold value for detection.\n",
    "I solved this by decreasing the minimum threshold for detection.\n",
    "\n",
    "Prior to ocr, the ocr_image module would clean the image of each cell. This led to certain numbers being cropped and others deleted entirely.\n",
    "I fixed this by increasing the kernel heigth and width for detecting cell boundaries. Now numbers are not cleaned out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably not needed\n",
    "\n",
    "from PIL import Image # same as above\n",
    "from pdf2image import convert_from_path # to convert pdf to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing other necessary packages\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 # image transformation\n",
    "import os\n",
    "import re\n",
    "import concurrent # for parallel instances\n",
    "import functools # for creating partial functions\n",
    "\n",
    "from io import StringIO # to convert string to csv\n",
    "import time # to measure time\n",
    "\n",
    "# to add the path where to search for modules\n",
    "import sys\n",
    "sys.path.append('/home/hennes/Internship/table_scanner')\n",
    "\n",
    "# Importing table_ocr modules \n",
    "from table_ocr import pdf_to_images\n",
    "from table_ocr import extract_tables\n",
    "from table_ocr import extract_cells\n",
    "from table_ocr import ocr_image\n",
    "from table_ocr import ocr_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/home/hennes/Downloads/2021 Form 20 Digitized Data\"\n",
    "pdflist = [pdf for pdf in glob.glob(folder+'/*') if pdf.endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing images\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    executor.map(pdf_to_images.pdf_to_images, pdflist)\n",
    "    \n",
    "end = time.time()\n",
    "print('OCR time =', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imglist = [img for img in glob.glob(folder+'/*') if img.endswith('.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing Table\n",
    "start = time.time()\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    executor.map(pdf_to_images.preprocess_img, imglist)\n",
    "\n",
    "end = time.time()\n",
    "print('OCR time =', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR time = 1.3536767959594727\n"
     ]
    }
   ],
   "source": [
    "# Extracting Table Image from PDF Page image\n",
    "start = time.time()\n",
    "\n",
    "imglist = [[img] for img in imglist]\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    executor.map(extract_tables.main, imglist)\n",
    "\n",
    "end = time.time()\n",
    "print('OCR time =', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of individual tables in subfolders \n",
    "\n",
    "dirlist = [x[0] for x in\n",
    "           [glob.glob(table) for table in \n",
    "           [directory+'/*' for directory in glob.glob(folder) if re.search(r'\\d$', directory)]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR time = 0.9223287105560303\n"
     ]
    }
   ],
   "source": [
    "# Extract individual cell images\n",
    "start = time.time()\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    executor.map(extract_cells.main, dirlist)\n",
    "\n",
    "end = time.time()\n",
    "print('OCR time =', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirlist = [directory+'*' for directory in glob.glob(folder+'/*/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cells folder is first in list of two objects in individual image folders.\n",
    "# That is why this code works (but only if executed after cells were extracted).\n",
    "\n",
    "dirlist = [directory+'*' for directory in glob.glob(folder+'/*/')]\n",
    "celllists = [glob.glob(cellfolder) for cellfolder in dirlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform OCR on each image\n",
    "\n",
    "os.environ['OMP_THREAD_LIMIT'] = '1'\n",
    "p_ocr_image = functools.partial(ocr_image.main, None)\n",
    "\n",
    "for image_list in celllists:\n",
    "    # perform OCR on each image\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(p_ocr_image, image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names for the individual pages\n",
    "\n",
    "pages = [filename for directory, filename in\n",
    "         [os.path.split(x) for x in glob.glob(folder) if not x.endswith(('.pdf', '.png'))]]\n",
    "\n",
    "# create list of alphabetically ordered lists of ocred files\n",
    "\n",
    "ocrlists = [sorted(y) for y in\n",
    "            [glob.glob(f'/home/hennes/Downloads/2021 Form 20 Digitized Data/{x}/cells/ocr_data/*.txt') for x in pages]]\n",
    "zippie = zip(pages, ocrlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting into csv fromAC1_Form20-005\n",
      "sucessfully appended data fromAC1_Form20-005\n",
      "Converting into csv fromAC1_Form20-000\n",
      "sucessfully appended data fromAC1_Form20-000\n",
      "Converting into csv fromAC2_Form20-013\n",
      "sucessfully appended data fromAC2_Form20-013\n",
      "Converting into csv fromAC2_Form20-016\n",
      "sucessfully appended data fromAC2_Form20-016\n",
      "Converting into csv fromAC2_Form20-006\n",
      "sucessfully appended data fromAC2_Form20-006\n",
      "Converting into csv fromAC1_Form20-003\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 15 fields in line 9, saw 16\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-791021e5a269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Skipping the first two rows because they have fewer columns than rest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Also useful for chaining of tables later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mgathered_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sucessfully appended data from\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2059\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 15 fields in line 9, saw 16\n"
     ]
    }
   ],
   "source": [
    "# Put OCRed str into csv\n",
    "gathered_data = []\n",
    "\n",
    "for y, x in zippie:\n",
    "    output = ocr_to_csv.main(x)\n",
    "    csv = StringIO(output)\n",
    "    print(\"Converting into csv from\" + y)\n",
    "    \n",
    "    # Turning csv into dataframe\n",
    "    # Skipping the first two rows because they have fewer columns than rest\n",
    "    # Also useful for chaining of tables later\n",
    "    df = pd.read_csv(csv,  header = None, skiprows=[0, 1])\n",
    "    gathered_data.append(df)\n",
    "    print(\"sucessfully appended data from\" + y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F,F,No,of Valid Vot,es Cast in Â£,avour of,F,F,F\\r\\n54,aes,,4,,ADHIKARY,ov,KMi,B4,s,s,,ors,Total\\r\\n76,60,470,5,1,406,1,0,1,0,4,888,0,7,895\\r\\n77,6,320,9,7,271,5,3,6,2,9,632,0,5,637\\r\\n78,62,588,60,2,17,4,1,0,3,3,678,0,7,685\\r\\n79,63,369,11,0,347,2,2,8,3,1,743,0,12,755\\r\\n80,64,259,13,0,316,5,0,2,1,1,597,0,3,600\\r\\n81,644),265,29,4,119,1,1,1,0,2,42.2,0,1,423\\r\\n82,65,135,8,1,289,4,0,3,1,4,445,0,3,448,0\\r\\n83,66,295,23,8,298,0,2,5,3,9,643,0,14,657,0\\r\\n84,67,482,56,3,473,5,1,8,3,7,1048,0,12,1060,[\\r\\n85,68,540,46,1,146,3,1,0,1,0,738,0,4,742,[\\r\\n86,684),131,10,1,146,2,0,0,2,0,292,0,0,292,[\\r\\n87,69,222,14,5,344,1,0,1,2,8,597,0,3,600,0\\r\\n88,694),220,18,3,343,4,1,2,0,0,591,0,4,595,0\\r\\n89,70,405,46,7,204,1,0,1,0,2,666,0,3,669,0\\r\\n90,70(A),243,40,1,309,2,2,1,2,2,602,0,1,603,0\\r\\n9,71,480,17,2,104,4,0,3,0,2,612,0,5,617,0\\r\\n92,71(4),356,17,5,186,6,0,6,2,4,582,0,13,595,[\\r\\n93,72,464,16,0,216,2,3,2,3,2,708,0,11,719,[|\\r\\n94,73,60,8,2,239,0,0,2,1,3,315,0,0,315,0\\r\\n5,74,262,30,1,528,2,4,4,6,5,842,0,12,854,0\\r\\n96,75,56,7,0,269,0,0,2,4,1,336,0,1,337,[\\r\\n97,76,322,32,5,311,1,1,8,2,3,685,0,8,693,[\\r\\n98,77,220,2,3,519,1,1,6,4,3,769,0,16,785,0\\r\\n99,78,320,23,5,186,2,3,3,1,3,546,0,3,549,0\\r\\n499,78A),232,10,2,250,1,0,2,0,2,499,0,2,501,077\\r\\n'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the Cell Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "directory, filename = os.path.split(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('/home/hennes/Downloads/2021 Form 20 Digitized Data/AC1_Form20-003/table-000.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# First image is blurred to reduce noise\n",
    "BLUR_KERNEL_SIZE = (9, 9)\n",
    "STD_DEV_X_DIRECTION = 0\n",
    "STD_DEV_Y_DIRECTION = 0\n",
    "blurred = cv2.GaussianBlur(image, BLUR_KERNEL_SIZE, STD_DEV_X_DIRECTION, STD_DEV_Y_DIRECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then thresholded to facilitate transformations\n",
    "MAX_COLOR_VAL = 255\n",
    "BLOCK_SIZE = 15\n",
    "SUBTRACT_FROM_MEAN = -2\n",
    "\n",
    "img_bin = cv2.adaptiveThreshold(\n",
    "    ~blurred,\n",
    "    MAX_COLOR_VAL,\n",
    "    cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "    cv2.THRESH_BINARY,\n",
    "    BLOCK_SIZE,\n",
    "    SUBTRACT_FROM_MEAN,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Vertical and Horizontal Lines\n",
    "vertical = horizontal = img_bin.copy()\n",
    "SCALE = 5\n",
    "image_width, image_height = horizontal.shape\n",
    "horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (int(image_width / SCALE), 1))\n",
    "horizontally_opened = cv2.morphologyEx(img_bin, cv2.MORPH_OPEN, horizontal_kernel)\n",
    "vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, int(image_height / SCALE)))\n",
    "vertically_opened = cv2.morphologyEx(img_bin, cv2.MORPH_OPEN, vertical_kernel)\n",
    "\n",
    "horizontally_dilated = cv2.dilate(horizontally_opened, cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1)))\n",
    "vertically_dilated = cv2.dilate(vertically_opened, cv2.getStructuringElement(cv2.MORPH_RECT, (1, 60)))\n",
    "\n",
    "mask = horizontally_dilated + vertically_dilated\n",
    "\n",
    "# Finding Contours of the lines\n",
    "contours, heirarchy = cv2.findContours(\n",
    "    mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE,\n",
    ")\n",
    "\n",
    "perimeter_lengths = [cv2.arcLength(c, True) for c in contours]\n",
    "epsilons = [0.05 * p for p in perimeter_lengths]\n",
    "approx_polys = [cv2.approxPolyDP(c, e, True) for c, e in zip(contours, epsilons)]\n",
    "\n",
    "# Filter out contours that aren't rectangular. Those that aren't rectangular\n",
    "# are probably noise.\n",
    "approx_rects = [p for p in approx_polys if len(p) == 4]\n",
    "bounding_rects = [cv2.boundingRect(a) for a in approx_polys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ..., 255, 255, 255],\n",
       "       [  0,   0,   0, ..., 255, 255, 255],\n",
       "       [  0,   0,   0, ..., 255, 255, 255],\n",
       "       ...,\n",
       "       [254, 254, 251, ..., 232, 136, 112],\n",
       "       [251, 254, 252, ..., 207,  87,  64],\n",
       "       [250, 254, 253, ..., 162,  59,  70]], dtype=uint8)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.drawContours(image, contours, -1, (0, 255, 0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('image', cv2.resize(image, (1066, 800)))\n",
    "k = cv2.waitKey(0) & 0xFF\n",
    "if k == 27:         # wait for ESC key to exit\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_cell_images_from_table(image):\n",
    "# First image is blurred to reduce noise\n",
    "    BLUR_KERNEL_SIZE = (17, 17)\n",
    "    STD_DEV_X_DIRECTION = 0\n",
    "    STD_DEV_Y_DIRECTION = 0\n",
    "    blurred = cv2.GaussianBlur(image, BLUR_KERNEL_SIZE, STD_DEV_X_DIRECTION, STD_DEV_Y_DIRECTION)\n",
    "# Then thresholded to facilitate transformations\n",
    "    MAX_COLOR_VAL = 255\n",
    "    BLOCK_SIZE = 15\n",
    "    SUBTRACT_FROM_MEAN = -2\n",
    "    \n",
    "    img_bin = cv2.adaptiveThreshold(\n",
    "        ~blurred,\n",
    "        MAX_COLOR_VAL,\n",
    "        cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "        cv2.THRESH_BINARY,\n",
    "        BLOCK_SIZE,\n",
    "        SUBTRACT_FROM_MEAN,\n",
    "    )\n",
    "# Finding Vertical and Horizontal Lines\n",
    "    vertical = horizontal = img_bin.copy()\n",
    "    SCALE = 5\n",
    "    image_width, image_height = horizontal.shape\n",
    "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (int(image_width / SCALE), 1))\n",
    "    horizontally_opened = cv2.morphologyEx(img_bin, cv2.MORPH_OPEN, horizontal_kernel)\n",
    "    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, int(image_height / SCALE)))\n",
    "    vertically_opened = cv2.morphologyEx(img_bin, cv2.MORPH_OPEN, vertical_kernel)\n",
    "    \n",
    "    horizontally_dilated = cv2.dilate(horizontally_opened, cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1)))\n",
    "    vertically_dilated = cv2.dilate(vertically_opened, cv2.getStructuringElement(cv2.MORPH_RECT, (1, 60)))\n",
    "    \n",
    "    mask = horizontally_dilated + vertically_dilated\n",
    "    \n",
    "# Finding Contours of the lines\n",
    "    contours, heirarchy = cv2.findContours(\n",
    "        mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE,\n",
    "    )\n",
    "    \n",
    "    perimeter_lengths = [cv2.arcLength(c, True) for c in contours]\n",
    "    epsilons = [0.05 * p for p in perimeter_lengths]\n",
    "    approx_polys = [cv2.approxPolyDP(c, e, True) for c, e in zip(contours, epsilons)]\n",
    "    \n",
    "    # Filter out contours that aren't rectangular. Those that aren't rectangular\n",
    "    # are probably noise.\n",
    "    approx_rects = [p for p in approx_polys if len(p) == 4]\n",
    "    bounding_rects = [cv2.boundingRect(a) for a in approx_polys]\n",
    "    \n",
    "    # Filter out rectangles that are too narrow or too short.\n",
    "    MIN_RECT_WIDTH = 30\n",
    "    MIN_RECT_HEIGHT = 10\n",
    "    bounding_rects = [\n",
    "        r for r in bounding_rects if MIN_RECT_WIDTH < r[2] and MIN_RECT_HEIGHT < r[3]\n",
    "    ]\n",
    "    \n",
    "    # The largest bounding rectangle is assumed to be the entire table.\n",
    "    # Remove it from the list. We don't want to accidentally try to OCR\n",
    "    # the entire table.\n",
    "    largest_rect = max(bounding_rects, key=lambda r: r[2] * r[3])\n",
    "    bounding_rects = [b for b in bounding_rects if b is not largest_rect]\n",
    "    \n",
    "    cells = [c for c in bounding_rects]\n",
    "    def cell_in_same_row(c1, c2):\n",
    "        c1_center = c1[1] + c1[3] - c1[3] / 2\n",
    "        c2_bottom = c2[1] + c2[3]\n",
    "        c2_top = c2[1]\n",
    "        return c2_top < c1_center < c2_bottom\n",
    "    \n",
    "    orig_cells = [c for c in cells]\n",
    "    rows = []\n",
    "    while cells:\n",
    "        first = cells[0]\n",
    "        rest = cells[1:]\n",
    "        cells_in_same_row = sorted(\n",
    "            [\n",
    "                c for c in rest\n",
    "                if cell_in_same_row(c, first)\n",
    "            ],\n",
    "            key=lambda c: c[0]\n",
    "        )\n",
    "    \n",
    "        row_cells = sorted([first] + cells_in_same_row, key=lambda c: c[0])\n",
    "        rows.append(row_cells)\n",
    "        cells = [\n",
    "            c for c in rest\n",
    "            if not cell_in_same_row(c, first)\n",
    "        ]\n",
    "    \n",
    "    # Sort rows by average height of their center.\n",
    "    def avg_height_of_center(row):\n",
    "        centers = [y + h - h / 2 for x, y, w, h in row]\n",
    "        return sum(centers) / len(centers)\n",
    "    \n",
    "    rows.sort(key=avg_height_of_center)\n",
    "    cell_images_rows = []\n",
    "    for row in rows:\n",
    "        cell_images_row = []\n",
    "        for x, y, w, h in row:\n",
    "            cell_images_row.append(image[y:y+h, x:x+w])\n",
    "        cell_images_rows.append(cell_images_row)\n",
    "    return cell_images_rows\n",
    "\n",
    "def main(f):\n",
    "    results = []\n",
    "    directory, filename = os.path.split(f)\n",
    "    table = cv2.imread(f, cv2.IMREAD_GRAYSCALE)\n",
    "    rows = extract_cell_images_from_table(table)\n",
    "    cell_img_dir = os.path.join(directory, \"cells\")\n",
    "    os.makedirs(cell_img_dir, exist_ok=True)\n",
    "    paths = []\n",
    "    for i, row in enumerate(rows):\n",
    "        for j, cell in enumerate(row):\n",
    "            cell_filename = \"{:03d}-{:03d}.png\".format(i, j)\n",
    "            path = os.path.join(cell_img_dir, cell_filename)\n",
    "            cv2.imwrite(path, cell)\n",
    "            paths.append(path)\n",
    "    return paths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
