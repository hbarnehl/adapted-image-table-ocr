{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages, defining AC list and Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import concurrent # for parallel instances\n",
    "import warnings\n",
    "\n",
    "# Defining Selection of Constituencies\n",
    "\n",
    "df = pd.read_excel('/home/hennes/Downloads/Book.xlsx')\n",
    "\n",
    "# get relevant pdf numbers\n",
    "\n",
    "worklist = df[df['Ready for Cleaning and Merging?'] == 'y']['Constituency number'].tolist()\n",
    "\n",
    "# give appropriate filename endings to items\n",
    "\n",
    "for idx, item in enumerate(worklist):\n",
    "    if len(str(item)) == 1:\n",
    "        worklist[idx] = f'AC00{item}.csv'\n",
    "    if len(str(item)) == 2:\n",
    "        worklist[idx] = f'AC0{item}.csv'\n",
    "    if len(str(item)) == 3:\n",
    "        worklist[idx] = f'AC{item}.csv'\n",
    "        \n",
    "worklist = tuple(worklist)\n",
    "\n",
    "# Defining Folders\n",
    "folder = '/home/hennes/Internship/constituencies/'\n",
    "save_folder = '/home/hennes/Internship/constituencies_edit/'\n",
    "old = '/home/hennes/Internship/old_files/'\n",
    "candidates = pd.read_excel('/home/hennes/Internship/Party_Data_2021.xlsx')\n",
    "PC_AC = set(sorted([folder.split('-')[0]+'-'+ folder.split('-')[1] for folder in next(os.walk(old))[1]]))\n",
    "PC_AC_dict = {e.split('-')[1]: e.split('-')[0] for e in PC_AC}\n",
    "constituencies = sorted([os.path.split(file)[-1] for file in glob.glob(folder+'*') if file.endswith(\".csv\")]) # list with all files\n",
    "constituencies = [file for file in constituencies if file.endswith(worklist)]\n",
    "\n",
    "# list of ACs belonging to individual PCs\n",
    "PClist = []\n",
    "unique_pc = set([e.split('-')[0] for e in PC_AC])\n",
    "edited_ac = [os.path.split(e)[1] for e in glob.glob(save_folder+'*')]\n",
    "\n",
    "for x in unique_pc:\n",
    "    PClist.append([e.split('-')[1] for e in PC_AC\n",
    "                   if e.split('-')[0] == x and e.split('-')[1]+'.csv' in edited_ac])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define options directly in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid literal for int() with base 10: ''\n",
      "Problem with AC033.csv. Jump to next one. \n",
      "\n",
      "invalid literal for int() with base 10: ''\n",
      "Problem with AC073.csv. Jump to next one. \n",
      "\n",
      "list index out of range\n",
      "Something went wrong with the column naming\n",
      "'NoneType' object has no attribute 'to_csv'\n",
      "Problem with AC154.csv. Jump to next one. \n",
      "\n",
      "list index out of range\n",
      "Something went wrong with the column naming\n",
      "'NoneType' object has no attribute 'to_csv'\n",
      "Problem with AC169.csv. Jump to next one. \n",
      "\n",
      "single positional indexer is out-of-bounds\n",
      "Problem with AC244.csv. Jump to next one. \n",
      "\n",
      "list index out of range\n",
      "Something went wrong with the column naming\n",
      "'NoneType' object has no attribute 'to_csv'\n",
      "Problem with AC253.csv. Jump to next one. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ignore pandas userwarnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(pipeline, constituencies)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituencies = sorted([os.path.split(file)[-1] for file in glob.glob(folder+'*') if file.endswith(\".csv\")]) # list with all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-5ddb7d2295ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconstituencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# list with all files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconstituencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-159-6cc8edd3fea1>\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(c)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_serial\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2021\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_digits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-159-6cc8edd3fea1>\u001b[0m in \u001b[0;36mcleaning\u001b[0;34m(constituency, candidate_df, year, A_serial, max_digits, max_value)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# Shift the values of the row to the right and report the new average standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mcollist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0msdlist_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0miloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtake_split_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m             \u001b[0;31m# We have to operate column-wise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer_split_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_single_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1698\u001b[0m                 \u001b[0;31m# We are setting multiple columns in a single row.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0milocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1700\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_single_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0milocs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_null_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_single_column\u001b[0;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0;31m# set the item, possibly having a dtype change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m             \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1813\u001b[0;31m             \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1814\u001b[0m             \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"BlockManager\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"setitem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     def quantile(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mfrom_blocks\u001b[0;34m(cls, blocks, axes)\u001b[0m\n\u001b[1;32m   1573\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1574\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "constituencies = sorted([os.path.split(file)[-1] for file in glob.glob(folder+'*') if file.endswith(\".csv\")]) # list with all files\n",
    "for c in constituencies:\n",
    "    pipeline(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of ACs belonging to individual PCs\n",
    "PClist = []\n",
    "unique_pc = set([e.split('-')[0] for e in PC_AC])\n",
    "edited_ac = [os.path.split(e)[1] for e in glob.glob(save_folder+'*')]\n",
    "\n",
    "for x in unique_pc:\n",
    "    PClist.append([e.split('-')[1] for e in PC_AC\n",
    "                   if e.split('-')[0] == x and e.split('-')[1]+'.csv' in edited_ac])\n",
    "\n",
    "for element in PClist:\n",
    "    try:\n",
    "        df = pd.concat(pd.read_csv(save_folder+e+'.csv') for e in element)\n",
    "        connect_party(df, element)\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(c):\n",
    "    try:\n",
    "        df = cleaning(c, candidates, A_serial= True, year=2021, max_digits = 5, max_value = 1600)\n",
    "        df.to_csv(save_folder+c, index=False)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f'Problem with {c}. Jump to next one. \\n')\n",
    "        return\n",
    "\n",
    "def cleaning(constituency, candidate_df, year, A_serial = None, max_digits = 4, max_value = None):\n",
    "    ''' This function performs the cleaning of scanned AC files.\n",
    "    \n",
    "    Options:\n",
    "    \n",
    "    year                This is relevant for the combination with the candidate & party table, which \n",
    "                        has a slightly different format every year.\n",
    "    \n",
    "    A-serial            Should be turned on for tables that contain one serial number with the letter\n",
    "                        \"A\" in them. It will ignore the steps required to handle this column and will\n",
    "                        carry out alternative steps where necessary.\n",
    "                        \n",
    "    max_digits          This specifies the maximum number of digits cells are allowed to have before\n",
    "                        they are turned into NAN. This is helpful because parliamentary elections have\n",
    "                        constituencies with more people, usually max 1500. These will be turned to NAN\n",
    "                        with standard options.\n",
    "                        \n",
    "    max_value           This specifies the maximum values cells are allowed to have before they get\n",
    "                        turned to NAN. This is helpful to delete noise and the summary rows.''' \n",
    "    \n",
    "    df = pd.read_csv(folder+constituency)\n",
    "\n",
    "    ################# Preliminary Cleaning #################\n",
    "    \n",
    "    # make F nan\n",
    "    repl_dict = {'F': np.NaN} \n",
    "    df = df.replace(repl_dict, regex=True)\n",
    "\n",
    "    # delete columns that have only have a few values in them. They are most likely useless.\n",
    "    df.dropna(thresh=len(df) - (len(df)/2), axis=1, inplace=True)\n",
    "    # delete rows that have more than 5 missing values\n",
    "    df.dropna(thresh = (len(df.columns)/1.4), axis = 0, inplace = True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # transform systematic errors\n",
    "    repl_dict = {'\\$':'5',\n",
    "                 'S':'5',\n",
    "                '\\(4\\)':'(A)',\n",
    "                '4\\)':'(A)',\n",
    "                '(\\(A\\))|(A\\))|(\\(A)|A':'A',\n",
    "                '(\\.0)$':'',\n",
    "                'v':'0',\n",
    "                '_':'',\n",
    "                '\\]':'',\n",
    "                '\\[':'',\n",
    "                '\\|':'',\n",
    "                '\\.':'',\n",
    "                '[\\(\\)]':'',\n",
    "                ' ': '',\n",
    "                '(?!A)\\D':''} \n",
    "    df = df.replace(repl_dict, regex=True)\n",
    "    \n",
    "    # This is needed to properly replace empty strings with NAN\n",
    "    df = df.replace(r'\\s+( +\\.)|#',np.nan,regex=True).replace('',np.nan)\n",
    "\n",
    "    # replace values with max_digits or more digits with NAN\n",
    "    regex_str = '\\d{' + str(max_digits) + ',}'\n",
    "    repl_dict = {regex_str: np.NaN} \n",
    "    df = df.replace(repl_dict, regex=True)\n",
    "\n",
    "    # delete rows that have more than 3 missing values\n",
    "    df.dropna(thresh = (len(df.columns)-3), axis = 0, inplace = True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    ############## Give Meaningful Column Names #################\n",
    "    \n",
    "    # The two columns with the highest numbers should be total valid votes and total votes.\n",
    "    # Total valid votes is to the left of total votes.\n",
    "    # first need to convert columns to int.\n",
    "    \n",
    "    if A_serial:\n",
    "        # In case of year with A-numbers, should only do that with the non-serial number columns.\n",
    "        # This mask selects all columns that do not have 'A' in them.\n",
    "        mask = df[[e for e in df.columns]].apply(lambda x:\n",
    "                                                 x.astype(str).str.contains(r'A', regex=True)).any(axis='index')\n",
    "\n",
    "        # Define two masks, one with all columns except serial number, second one only serial number\n",
    "        serial = [df.iloc[:,2].name]\n",
    "        not_serial = df.loc[:,df.columns != serial[0]].columns.tolist()\n",
    "\n",
    "        # convert all remaining characters to numeric or nan\n",
    "        for col in not_serial:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df[col] = df[col].astype(float)\n",
    "            if max_value:\n",
    "                df[df[col] > max_value] = np.nan\n",
    "                df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    else:\n",
    "        # convert all remaining characters to numeric or nan\n",
    "        for col in df.columns.tolist():\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df[col] = df[col].astype(float)\n",
    "    \n",
    "        if max_value:\n",
    "            df[df > max_value] = np.nan\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # name column with highest median value 'total votes'    \n",
    "    df.rename(columns = {df.median().idxmax(axis=1):'total'}, inplace=True)\n",
    "\n",
    "    # exclude 'total votes' and name the one with second highest median 'total_valid'\n",
    "    columns = [col for col in df.columns if not col.startswith('total')]\n",
    "    df.rename(columns = {df[columns].median().idxmax(axis=1):'total_valid'}, inplace=True)\n",
    "    \n",
    "    # sometimes total and total valid will be switched.\n",
    "    # Check if column three places to left of total_valid is called total\n",
    "    # If so, switch their names\n",
    "    if df.columns.tolist()[df.columns.tolist().index('total_valid')-3] == 'total':\n",
    "        df.rename(columns = {'total': 'total_valid', 'total_valid':'total'}, inplace=True)\n",
    "\n",
    "    # get names of columns one and two places to right of 'total valid'\n",
    "    try:\n",
    "        sublist = ['total_valid']\n",
    "        rejected = df.columns.tolist()[(df.columns.get_indexer(sublist)+1)[0]]\n",
    "        nota = df.columns.tolist()[(df.columns.get_indexer(sublist)+2)[0]]\n",
    "        first = df.columns.tolist()[0]\n",
    "        second = df.columns.tolist()[1]\n",
    "        third = df.columns.tolist()[2]\n",
    "        \n",
    "        # in 2019, sometimes there is only one serial column. To find out when that is the case,\n",
    "        # I calculate the median euclidian distance between columns 2 and 3. If it is smaller than\n",
    "        # 30, then these two are probably both serial numbers.\n",
    "        if not A_serial:\n",
    "            distance = []\n",
    "            for row in df.index.tolist():\n",
    "                distance.append(abs(df.loc[row, second] - df.loc[row, third]))\n",
    "\n",
    "            row_dist = np.nanmedian(distance)\n",
    "            \n",
    "            if row_dist < 31:\n",
    "                # rename first column 'page_idx' and other columns\n",
    "                df.rename(columns={rejected:'rejected',\n",
    "                               nota:'nota',\n",
    "                               first:'page_idx',\n",
    "                               second:'serial_1',\n",
    "                               third:'serial'}, inplace = True)\n",
    "            elif row_dist >= 31:\n",
    "                # rename first column 'page_idx' and other columns\n",
    "                df.rename(columns={rejected:'rejected',\n",
    "                               nota:'nota',\n",
    "                               first:'page_idx',\n",
    "                               second:'serial'}, inplace = True)\n",
    "                \n",
    "        else:\n",
    "            df.rename(columns={rejected:'rejected',\n",
    "               nota:'nota',\n",
    "               first:'page_idx',\n",
    "               second:'serial_1',\n",
    "               third:'serial'}, inplace = True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Something went wrong with the column naming')\n",
    "        return\n",
    "\n",
    "        # rename first column 'page_idx' and other columns\n",
    "        df.rename(columns={rejected:'rejected',\n",
    "                           nota:'nota',\n",
    "                           first:'page_idx',\n",
    "                           second:'serial_1',\n",
    "                           third:'serial'}, inplace = True)\n",
    "\n",
    "\n",
    "    # delete all rows in which no cell has more than two digits (also accounting for .0)\n",
    "\n",
    "    mask = df.apply(lambda x: x.astype(str).str.contains(r'^\\d{,2}(\\.0)?$', regex=True)).all(axis=1)\n",
    "    df = df[~mask]\n",
    "    # sometimes that does not work, so to be sure: keep only rows with sum of >300\n",
    "    df = df.loc[df.sum(1) >= 350]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    ############## correcting columns wrongly shifted #######################\n",
    "\n",
    "    # Identify all rows that have NAN in the column furthest right\n",
    "    \n",
    "    if A_serial:\n",
    "        # and which do not have an 'A' in the serial column. Those should not be moved\n",
    "        rowlist = df[(df.iloc[:,-1].isna()) & (~df['serial'].str.contains('A', na=False))].index.tolist()\n",
    "    else:\n",
    "        rowlist = df[(df.iloc[:,-1].isna())].index.tolist()\n",
    "\n",
    "    # Calculate how many standard deviations all values of each row are away from the average of the respective columns \n",
    "    for row in rowlist:\n",
    "        collist = df.dtypes[df.dtypes == float].index.tolist()\n",
    "        sdlist_old = []\n",
    "        for col in collist:\n",
    "            sdlist_old.append(abs(df.loc[row, col] - df[col].mean()) / df[col].std()) ## abs returns positive numbers\n",
    "\n",
    "    # Compute the average standard deviation for each of these rows \n",
    "        rowsd_old = np.nanmedian(sdlist_old)\n",
    "\n",
    "    # Shift the values of the row to the right and report the new average standard deviation \n",
    "        df1 = df.copy(deep=True)\n",
    "        df1.loc[row, :] = df1.loc[row, :].shift(1, axis=0)\n",
    "        collist = df1.dtypes[df1.dtypes == float].index.tolist()\n",
    "        sdlist_new = []\n",
    "        for col in collist:\n",
    "            sdlist_new.append(abs(df1.loc[row, col] - df[col].mean()) / df1[col].std())\n",
    "\n",
    "        rowsd_new = np.nanmedian(sdlist_new)\n",
    "\n",
    "    # Take over the shift if the new SD is smaller than the old SD\n",
    "\n",
    "        if rowsd_old > rowsd_new:\n",
    "            df.loc[row] = df1.loc[row]\n",
    "            \n",
    "            if A_serial:\n",
    "                # make sure that all except serial are still float\n",
    "                serial = [df.iloc[:,2].name]\n",
    "                not_serial = df.loc[:,df.columns != serial[0]].columns.tolist()\n",
    "\n",
    "                # then convert all remaining characters to numeric or nan\n",
    "                for col in not_serial:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    df[col] = df[col].astype(float)\n",
    "\n",
    "    ####################### Correct Serial Numbers #############################\n",
    "\n",
    "    # For errors without a gap (no row is missing)\n",
    "\n",
    "    # Idea is: if (n)+1 does not equal (n+1), then see if (n-1)+2 equals (n+1)\n",
    "    # This logic is extended for up to 5 numbers ahead of n. In this way, gaps \n",
    "    # of up to 4 numbers will be bridged. At the same time, there will be no\n",
    "    # interpolation if there is no clean continuation of integers.\n",
    "\n",
    "    for n in df.index.tolist()[1:]:\n",
    "        try:\n",
    "            if df.iloc[n,1] != df.iloc[n-1,1]+1:\n",
    "                if df.iloc[n-1,1]+2 == df.iloc[n+1,1]:\n",
    "                    df.iloc[n,1] = df.iloc[n-1,1]+1\n",
    "                if df.iloc[n-1,1]+3 == df.iloc[n+2,1]:\n",
    "                    df.iloc[n,1] = df.iloc[n-1,1]+1\n",
    "                    df.iloc[n+1,1] = df.iloc[n-1,1]+2\n",
    "                if df.iloc[n-1,1]+4 == df.iloc[n+3,1]:\n",
    "                    df.iloc[n,1] = df.iloc[n-1,1]+1\n",
    "                    df.iloc[n+1,1] = df.iloc[n-1,1]+2\n",
    "                    df.iloc[n+2,1] = df.iloc[n-1,1]+3\n",
    "                if df.iloc[n-1,1]+5 == df.iloc[n+4,1]:\n",
    "                    df.iloc[n,1] = df.iloc[n-1,1]+1\n",
    "                    df.iloc[n+1,1] = df.iloc[n-1,1]+2\n",
    "                    df.iloc[n+2,1] = df.iloc[n-1,1]+3\n",
    "                    df.iloc[n+3,1] = df.iloc[n-1,1]+4\n",
    "                if df.iloc[n-1,1]+6 == df.iloc[n+5,1]:\n",
    "                    df.iloc[n,1] = df.iloc[n-1,1]+1\n",
    "                    df.iloc[n+1,1] = df.iloc[n-1,1]+2\n",
    "                    df.iloc[n+2,1] = df.iloc[n-1,1]+3\n",
    "                    df.iloc[n+3,1] = df.iloc[n-1,1]+4\n",
    "                    df.iloc[n+4,1] = df.iloc[n-1,1]+5\n",
    "        except:\n",
    "            None\n",
    "\n",
    "    # For errors with a gap (a row is missing)\n",
    "    # define index of last row of each page\n",
    "    last = [x-1 for x in df[df['page_idx']==0].index.tolist()[1:]]\n",
    "\n",
    "    # if (n)+1 does not equal (n+1), then see if (n-1)+2 equals (n+1)\n",
    "    for n in last:\n",
    "        try:\n",
    "            if df.iloc[n,1] != df.iloc[n-1,1]+1:\n",
    "                df.iloc[n,1] = df.iloc[n-1,1]+1\n",
    "        except:\n",
    "            None\n",
    "    \n",
    "    # for errors at beginning of pages\n",
    "\n",
    "    for n in df[df['page_idx']==0].index.tolist()[1:]:\n",
    "        try:\n",
    "            if df.iloc[n,1] != df.iloc[n+1,1]-1:\n",
    "                df.iloc[n,1] = df.iloc[n+1,1]-1\n",
    "        except:\n",
    "            None\n",
    "            \n",
    "    ########################## Correct Serial Numbers with A ##############################\n",
    "    \n",
    "    if A_serial: # only do this if there is indeed a column with A\n",
    "        # fill nan with empty strings\n",
    "        df.iloc[:,2].fillna('', inplace = True)\n",
    "        # set cells with only an \"A\" to \"0A\"\n",
    "        mask = df.loc[:,\"serial\"]=='A'\n",
    "        df.loc[mask, \"serial\"] = \"0\"\n",
    "        # set cells with empty string to \"0\"\n",
    "        mask = df.loc[:,\"serial\"]==''\n",
    "        df.loc[mask, \"serial\"] = \"0\"\n",
    "        # for all rows except last row\n",
    "        for n in df.index.tolist()[1:-1]:\n",
    "            \n",
    "            # preceding and following row should not contain any A\n",
    "            if not (\"A\" in \"\".join([str(df.iloc[n-1,2]), str(df.iloc[n+1,2])]) and not\n",
    "                # n should not be identical to preceding or following row\n",
    "                int(''.join(c for c in str(df.iloc[n,2]) if c.isdigit())) ==\n",
    "                int(''.join(c for c in str(df.iloc[n+1,2]) if c.isdigit())) and not\n",
    "                int(''.join(c for c in str(df.iloc[n,2]) if c.isdigit())) ==\n",
    "                int(''.join(c for c in str(df.iloc[n-1,2]) if c.isdigit()))):\n",
    "                # if n-1 == (n+1)-1, then make n = n-1+A\n",
    "                if (int(''.join(c for c in str(df.iloc[n-1,2]) if c.isdigit())) ==\n",
    "                    int(''.join(c for c in str(df.iloc[n+1,2]) if c.isdigit()))-1):\n",
    "                    df.iloc[n,2] = ''.join([str(int(''.join(c for c in str(df.iloc[n-1,2]) if c.isdigit()))), 'A'])\n",
    "                # in cases where n-1 = n+1 -2 (then n should not have an A and simply be n-1 + 1)\n",
    "                elif (int(''.join(c for c in str(df.iloc[n-1,2]) if c.isdigit())) ==\n",
    "                      int(''.join(c for c in str(df.iloc[n+1,2]) if c.isdigit()))-2):\n",
    "                    df.iloc[n,2] = int(''.join(c for c in str(df.iloc[n-1,2]) if c.isdigit()))+1\n",
    "            \n",
    "            # if the former row ends with an A and does not start with an A\n",
    "            if str(df.iloc[n-1,2]).endswith(\"A\") and not str(df.iloc[n-1,2]).startswith(\"A\"):\n",
    "                # then just make n = former row + 1\n",
    "                df.iloc[n,2] = int(''.join(c for c in df.iloc[n-1,2] if c.isdigit()))+1\n",
    "            # if the next row ends with an A and does not start with an A\n",
    "            elif str(df.iloc[n+1,2]).endswith(\"A\") and not str(df.iloc[n+1,2]).startswith(\"A\"):\n",
    "                # then just make n = next row\n",
    "                df.iloc[n,2] = int(''.join(c for c in df.iloc[n+1,2] if c.isdigit()))\n",
    "            \n",
    "\n",
    "       \n",
    "\n",
    "    ################# Rename Candidate Columns According to Parties #######################\n",
    "    if year == 2021:\n",
    "        # get appropriate constituency number\n",
    "        con_n = float(re.sub(r'AC0*', '' ,constituency.split('.')[0]))\n",
    "\n",
    "        # define df excluding NOTA, only current constituency\n",
    "        dat = candidate_df[(candidate_df['PARTY']!= 'NOTA')\n",
    "                           & (candidate_df['AC NO.'] == con_n)][['AC NO.', 'PARTY', 'TOTAL']]\n",
    "        # create column with rank of party per constituency\n",
    "        dat['rank'] = dat.groupby('AC NO.').rank(ascending=False)\n",
    "        # get number of candidates\n",
    "        n_candidates = len(dat)\n",
    "        # create dict with party value pair\n",
    "        rank_party = pd.Series(dat.PARTY.values,index=dat['rank']).to_dict()\n",
    "\n",
    "        # create dictionary with key = column name, value = rank\n",
    "        serial = df.columns.get_indexer(['serial'])[0]\n",
    "        column_rank = df.iloc[:,serial+1:serial+(n_candidates+1)]\\\n",
    "            .agg(func=np.sum)\\\n",
    "            .rank(ascending=False)\\\n",
    "            .to_dict()\n",
    "\n",
    "        # Renaming column according to rank\n",
    "        rename_dict={}\n",
    "        for col, rank in column_rank.items():\n",
    "            rename_dict.update({col:rank_party.get(rank)}) \n",
    "        df.rename(columns=rename_dict, inplace=True)\n",
    "    \n",
    "    ################# create constituency number column #######################\n",
    "    \n",
    "    df['ac'] = re.findall(r'[1-9][0-9]*' ,constituency)[0]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def connect_party(df, ACs):\n",
    "    \n",
    "    # get appropriate constituency number\n",
    "    con_n = PC_AC_dict[ACs[0].split('.')[0]].split('C')[-1].replace('0', '')\n",
    "\n",
    "    # define df excluding NOTA, only current constituency\n",
    "    dat = candidates[(candidates['Party']!= 'NOTA') & (candidates['Constituency_No'] == int(con_n))]\\\n",
    "    [['Constituency_No', 'Party', 'Votes', 'Constituency_Name']]\n",
    "\n",
    "    dat['Votes'] = pd.to_numeric(dat['Votes'], errors='coerce')\n",
    "    # create column with rank of party per constituency\n",
    "    dat['rank'] = dat.groupby('Constituency_No').rank(ascending=False)\n",
    "    # get number of candidates\n",
    "    n_candidates = len(dat)\n",
    "    # create dict with party value pair\n",
    "    rank_party = pd.Series(dat.Party.values,index=dat['rank']).to_dict()\n",
    "\n",
    "    # create dictionary with key = column name, value = rank\n",
    "    serial = df.columns.get_indexer(['serial'])[0]\n",
    "    column_rank = df.iloc[:,serial+1:serial+(n_candidates+1)]\\\n",
    "        .agg(func=np.sum)\\\n",
    "        .rank(ascending=False)\\\n",
    "        .to_dict()\n",
    "\n",
    "    df['pc'] = con_n\n",
    "\n",
    "    # Renaming column according to rank\n",
    "    rename_dict={}\n",
    "    for col, rank in column_rank.items():\n",
    "        rename_dict.update({col:rank_party.get(rank)}) \n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "    \n",
    "    dflist = []\n",
    "    for x in df['ac'].unique().tolist():\n",
    "        acn = \"{:03d}\".format(int(x))\n",
    "        df[df['ac']==x].to_csv(f'{save_folder}AC{acn}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
